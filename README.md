üî¨ Deep Research Agent
Type: Agentic AI System | Tech Stack: Python, OpenAI Agents SDK, Serper API, Gradio

Status: Production-Ready MVP (v1.0)

üìñ Project Overview
This project is a fully autonomous, Agentic AI Research Assistant designed to automate the labor-intensive process of information gathering, synthesis, and report generation.

Unlike standard chatbots that rely on pre-trained static knowledge, this agent actively researches real-time data, plans its own search strategy, and compiles exhaustive, cited reports on complex topics‚Äîmimicking the workflows of senior analysts.

üõ†Ô∏è Architecture
The system is built on a Multi-Agent Orchestration Pipeline, utilizing a specialized architecture to ensure depth, speed, and reliability.

Core Components
The Planner (Strategic Orchestrator)
Role: Deconstructs the user's broad query into 10-15 distinct, high-value search vectors.
Technique: Uses Chain-of-Thought (CoT) prompting to identify "Unknown Unknowns"‚Äîgaps in knowledge that a naive search might miss.
Output: A JSON-based SearchPlan covering definitions, mechanisms, limitations, and recent trends.
The Analyst (Information Extraction)
Role: Processes raw search data to filter noise (SEO fluff, ads) and distill high-signal facts.
Optimization: Designed to preserve technical nuance and context, ensuring the final writer isn't "hallucinating" based on marketing blurbs.
The Technical Writer (Synthesis)
Role: Weaves distilled findings into a cohesive, human-readable narrative.
Capabilities: Generates Markdown-formatted reports with tables, citations, and structured headers without losing the "story" of the data.
Search Engine (Real-time Retrieval)
Implementation: Integrated Serper.dev for high-availability Google Search results.
Why: Overcame reliability issues with free search APIs (DuckDuckGo) that frequently block cloud IPs, ensuring 99.9% uptime on cloud infrastructure.
üß© Engineering Challenges & Solutions
Developing a functional research agent required solving several non-trivial engineering problems.

Challenge 1: The "Brevity Bias" of LLMs
The Problem:Standard Large Language Models (like GPT-4) are fine-tuned for conversational brevity. When asked to "research," they default to 200-300 word summaries, lacking the depth required for professional analysis.

The Solution:I implemented Constraint-Based Prompting. Instead of generic "be detailed" instructions, I enforced a strict structural schema:

Mandatory Sections: Definition, Mechanisms, Applications, Limitations, Future Outlook.
Formatting Rules: Enforced Markdown Tables for comparisons and specific Citation protocols [1].
Outcome: The agent now generates 1,500+ word reports with comprehensive coverage.
Challenge 2: Sequential Latency
The Problem:A naive implementation iterates through search queries one by one (1s delay + network time per query). With 15 queries, the user waits 30+ seconds before processing even begins.

The Solution:I refactored the pipeline using Python's asyncio.gather.

All 15 search queries are dispatched concurrently.
The system waits only for the slowest response.
Outcome: Reduced the research phase from ~30s to ~3s, drastically improving User Experience (UX).
Challenge 3: "Snippet" vs. "Knowledge" Gap
The Problem:Search APIs return short "snippets" (metadata). Feeding only snippets to the Writer agent results in superficial reports, as the AI lacks the full context of the source material.

The Solution:

Implemented a Multi-Pass Retrieval system:
Pass 1 (Broad): Serper retrieves high-level URLs.
Pass 2 (Deep): Note: Current MVP uses Snippets; next phase implements Web Scraping.
Current Strategy: The Analyst agent performs "Context Compression," extracting maximum logic from limited snippets to feed the Writer.
Challenge 4: Deployment on Cloud Infrastructure (Hugging Face)
The Problem:Deploying asynchronous Python agents on serverless platforms (like Hugging Face Spaces) introduced:

Import Errors: Module resolution issues in containerized environments.
IP Blocking: Free search APIs (DuckDuckGo) blocking cloud IP addresses.
The Solution:

Environment Hardcoding: Used sys.path.append to ensure module visibility in the container.
Provider Switching: Migrated from DuckDuckGo to Serper.dev (Google via API), a paid but highly reliable service that guarantees consistent data retrieval on cloud IPs.
üöÄ Tech Stack
LLM Orchestration: openai-agents (Agent SDK)
Frontend: Gradio (Python UI)
Search API: Serper.dev (Google Search Integration)
Async Runtime: asyncio (Parallel Task Execution)
Data Models: Pydantic (Structured Outputs)
Environment: Python 3.10+, Docker (Hugging Face Spaces)


üîÆ Future Roadmap
To bridge the gap between this MVP and industry leaders (Perplexity, OpenAI Deep Research), the following iterations are planned:

Full-Text Scraping: Integrating BeautifulSoup or Jina Reader to scrape the entire text of top sources, moving beyond snippet limitations.
Streaming Responses: Implementing token-streaming in the UI to show the report generating in real-time, increasing perceived intelligence.
Re-Ranking: Adding a cross-encoder model to surgically extract the most relevant sentences from 100+ pages of text.

This is a portfolio project demonstrating autonomous agent design and full-stack Python deployment.
